#!/usr/bin/env python3
"""
VLM+RLè”åˆè®­ç»ƒè„šæœ¬
ç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹å’Œå¼ºåŒ–å­¦ä¹ çš„æœºæ¢°è‡‚æ™ºèƒ½æ§åˆ¶è®­ç»ƒ
"""

import os
import sys
import numpy as np
import torch
import logging
from typing import Dict, List, Tuple
import time

# è®¾ç½®ç¯å¢ƒå˜é‡
cache_dir = "./vlm_models"
os.environ['HF_HOME'] = cache_dir
os.environ['TRANSFORMERS_CACHE'] = cache_dir

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class VLMRLTrainer:
    """
    VLM+RLè”åˆè®­ç»ƒå™¨
    """
    
    def __init__(self, config: Dict = None):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            config: è®­ç»ƒé…ç½®
        """
        self.config = config or self._get_default_config()
        self.vlm_processor = None
        self.rl_agent = None
        self.environment = None
        
        logger.info("VLM+RLè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _get_default_config(self) -> Dict:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            "vlm": {
                "model_name": "Qwen/Qwen-VL-Chat",
                "device": "cuda" if torch.cuda.is_available() else "cpu",  # æ˜ç¡®æŒ‡å®šè®¾å¤‡
                "cache_dir": cache_dir
            },
            "rl": {
                "algorithm": "PPO",
                "learning_rate": 1e-4,        # é™ä½å­¦ä¹ ç‡ï¼Œæ›´ç¨³å®š
                "total_timesteps": 500000,    # å¢åŠ è®­ç»ƒæ­¥æ•°
                "batch_size": 256,            # å¢åŠ æ‰¹æ¬¡å¤§å°
                "n_steps": 2048,              # ä¿æŒä¸å˜
                "n_epochs": 8,                # å‡å°‘è®­ç»ƒè½®æ•°
                "gamma": 0.99,                # æŠ˜æ‰£å› å­
                "gae_lambda": 0.95,           # GAEå‚æ•°
                "clip_range": 0.2             # PPOè£å‰ªèŒƒå›´
            },
            "training": {
                "max_episodes": 2000,         # å¢åŠ episodeæ•°é‡
                "episode_length": 300,        # å¢åŠ episodeé•¿åº¦
                "save_interval": 50           # å¢åŠ ä¿å­˜é—´éš”
            }
        }
    
    def setup_components(self):
        """è®¾ç½®VLMå’ŒRLç»„ä»¶"""
        try:
            logger.info("æ­£åœ¨è®¾ç½®VLMç»„ä»¶...")
            from vlm_module import VLMProcessor
            self.vlm_processor = VLMProcessor(
                model_name=self.config["vlm"]["model_name"],
                device=self.config["vlm"]["device"]
            )
            logger.info("âœ… VLMç»„ä»¶è®¾ç½®å®Œæˆ")
            
            logger.info("æ­£åœ¨è®¾ç½®RLç»„ä»¶...")
            from rl_module import RLAgent, PandaEnvironment
            self.environment = PandaEnvironment()
            self.rl_agent = RLAgent(
                input_dim=self.environment.observation_space.shape[0],
                output_dim=self.environment.action_space.shape[0],
                algorithm=self.config["rl"]["algorithm"]
            )
            self.rl_agent.set_environment(self.environment)
            logger.info("âœ… RLç»„ä»¶è®¾ç½®å®Œæˆ")
            
        except Exception as e:
            logger.error(f"ç»„ä»¶è®¾ç½®å¤±è´¥: {e}")
            raise
    
    def train_single_episode(self, episode: int) -> Dict:
        """
        è®­ç»ƒå•ä¸ªepisode
        
        Args:
            episode: episodeç¼–å·
            
        Returns:
            Dict: episodeç»“æœ
        """
        logger.info(f"å¼€å§‹è®­ç»ƒEpisode {episode}")
        
        # é‡ç½®ç¯å¢ƒ
        obs, info = self.environment.reset()
        episode_reward = 0
        episode_steps = 0
        episode_success = False
        
        # è·å–ç›¸æœºå›¾åƒ
        camera_image = self.environment.get_camera_image()
        
        # éšæœºé€‰æ‹©æŒ‡ä»¤
        import random
        instructions = [
            "ç§»åŠ¨åˆ°çº¢è‰²ç«‹æ–¹ä½“ä¸Šæ–¹",
            "ç§»åŠ¨åˆ°è“è‰²åœ†å½¢ä¸Šæ–¹", 
            "ç§»åŠ¨åˆ°ç»¿è‰²æ–¹å—ä¸Šæ–¹",
            "ç§»åŠ¨åˆ°é»„è‰²çƒä½“ä¸Šæ–¹",
            "æŠ“å–çº¢è‰²ç«‹æ–¹ä½“åˆ°å³è¾¹",
            "æŠ“å–è“è‰²åœ†å½¢åˆ°å·¦è¾¹",
            "æŠ“å–ç»¿è‰²æ–¹å—åˆ°ä¸Šè¾¹",
            "æŠ“å–é»„è‰²çƒä½“åˆ°å‰é¢"
        ]
        instruction = random.choice(instructions)
        
        # ä½¿ç”¨VLMå¤„ç†å›¾åƒå’ŒæŒ‡ä»¤
        try:
            vlm_result = self.vlm_processor.process_instruction(camera_image, instruction)
            logger.info(f"VLMå¤„ç†ç»“æœ: {vlm_result}")
        except Exception as e:
            logger.error(f"VLMå¤„ç†å¤±è´¥: {e}")
            # ä¸å†ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆï¼Œç›´æ¥æŠ›å‡ºå¼‚å¸¸ä»¥ä¾¿è¯Šæ–­é—®é¢˜
            raise ValueError(f"VLMå¤„ç†å¤±è´¥ï¼Œéœ€è¦è¯Šæ–­é—®é¢˜: {e}")
        
        # æ ¹æ®ä»»åŠ¡ç±»å‹è®¾ç½®ç›®æ ‡ä½ç½®
        if "task_type" in vlm_result:
            task_type = vlm_result["task_type"]
            pickup_position = vlm_result.get("pickup_position", [0.5, 0.0, 0.35])
            place_position = vlm_result.get("place_position", [0.5, 0.0, 0.35])
            
            # è®¾ç½®ä»»åŠ¡é…ç½®
            self.environment.set_task_config(
                task_type=task_type,
                pickup_position=pickup_position,
                place_position=place_position
            )
            
            if task_type == "move_to":
                logger.info(f"VLMè¯†åˆ«ç§»åŠ¨ä»»åŠ¡ï¼Œç›®æ ‡ä½ç½®: {pickup_position}")
            elif task_type == "grasp_and_move":
                logger.info(f"VLMè¯†åˆ«æŠ“å–ä»»åŠ¡ï¼ŒæŠ“å–ä½ç½®: {pickup_position}, æ”¾ç½®ä½ç½®: {place_position}")
        else:
            # å…¼å®¹æ—§æ ¼å¼
            if "world_position" in vlm_result:
                target_position = vlm_result["world_position"]
            else:
                target_position = [0.5, 0.0, 0.35]
            
            # è®¾ç½®ä¸ºç§»åŠ¨ä»»åŠ¡
            self.environment.set_task_config(
                task_type="move_to",
                pickup_position=target_position
            )
            logger.info(f"ä½¿ç”¨å…¼å®¹æ¨¡å¼ï¼Œç›®æ ‡ä½ç½®: {target_position}")
        
        # æ‰§è¡Œepisode
        for step in range(self.config["training"]["episode_length"]):
            # RLæ™ºèƒ½ä½“é€‰æ‹©åŠ¨ä½œ
            action, _ = self.rl_agent.predict(obs, deterministic=False)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            obs, reward, terminated, truncated, info = self.environment.step(action)
            done = terminated or truncated
            
            episode_reward += reward
            episode_steps += 1
            
            # æ£€æŸ¥æ˜¯å¦æˆåŠŸ
            if info.get("success", False):
                episode_success = True
                logger.info(f"Episode {episode} åœ¨ç¬¬{step}æ­¥æˆåŠŸå®Œæˆ")
                break
            
            if done:
                break
        
        episode_result = {
            "episode": episode,
            "reward": episode_reward,
            "steps": episode_steps,
            "success": episode_success,
            "vlm_result": vlm_result,
            "instruction": instruction
        }
        
        logger.info(f"Episode {episode} å®Œæˆ: æŒ‡ä»¤='{instruction}', å¥–åŠ±={episode_reward:.2f}, æ­¥æ•°={episode_steps}, æˆåŠŸ={episode_success}")
        return episode_result
    
    def train(self):
        """æ‰§è¡Œå®Œæ•´è®­ç»ƒ"""
        logger.info("å¼€å§‹VLM+RLè”åˆè®­ç»ƒ")
        
        # è®¾ç½®ç»„ä»¶
        self.setup_components()
        
        # è®­ç»ƒç»Ÿè®¡
        training_stats = {
            "episodes": [],
            "total_rewards": [],
            "success_rate": [],
            "avg_steps": []
        }
        
        # å¼€å§‹è®­ç»ƒ
        for episode in range(self.config["training"]["max_episodes"]):
            try:
                # è®­ç»ƒå•ä¸ªepisode
                episode_result = self.train_single_episode(episode)
                
                # è®°å½•ç»Ÿè®¡
                training_stats["episodes"].append(episode)
                training_stats["total_rewards"].append(episode_result["reward"])
                training_stats["success_rate"].append(episode_result["success"])
                training_stats["avg_steps"].append(episode_result["steps"])
                
                # å®šæœŸä¿å­˜æ¨¡å‹
                if (episode + 1) % self.config["training"]["save_interval"] == 0:
                    self.save_models(episode + 1)
                    self.print_training_summary(training_stats)
                
                # çŸ­æš‚ä¼‘æ¯
                time.sleep(0.1)
                
            except Exception as e:
                logger.error(f"Episode {episode} è®­ç»ƒå¤±è´¥: {e}")
                continue
        
        # è®­ç»ƒå®Œæˆ
        logger.info("VLM+RLè”åˆè®­ç»ƒå®Œæˆ")
        self.save_models("final")
        self.print_final_summary(training_stats)
    
    def save_models(self, suffix: str):
        """ä¿å­˜æ¨¡å‹"""
        try:
            # ä¿å­˜RLæ¨¡å‹
            rl_model_path = f"models/rl_agent_{suffix}.zip"
            os.makedirs("models", exist_ok=True)
            self.rl_agent.save(rl_model_path)
            logger.info(f"RLæ¨¡å‹å·²ä¿å­˜åˆ°: {rl_model_path}")
            
            # ä¿å­˜è®­ç»ƒé…ç½®
            config_path = f"models/training_config_{suffix}.json"
            import json
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(self.config, f, indent=2, ensure_ascii=False)
            logger.info(f"è®­ç»ƒé…ç½®å·²ä¿å­˜åˆ°: {config_path}")
            
        except Exception as e:
            logger.error(f"æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
    
    def print_training_summary(self, stats: Dict):
        """æ‰“å°è®­ç»ƒæ‘˜è¦"""
        recent_episodes = stats["episodes"][-10:]  # æœ€è¿‘10ä¸ªepisode
        recent_rewards = stats["total_rewards"][-10:]
        recent_success = stats["success_rate"][-10:]
        
        avg_reward = np.mean(recent_rewards)
        success_rate = np.mean(recent_success) * 100
        
        logger.info(f"è®­ç»ƒæ‘˜è¦ (æœ€è¿‘10ä¸ªepisode):")
        logger.info(f"  å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
        logger.info(f"  æˆåŠŸç‡: {success_rate:.1f}%")
    
    def print_final_summary(self, stats: Dict):
        """æ‰“å°æœ€ç»ˆè®­ç»ƒæ‘˜è¦"""
        total_episodes = len(stats["episodes"])
        avg_reward = np.mean(stats["total_rewards"])
        success_rate = np.mean(stats["success_rate"]) * 100
        avg_steps = np.mean(stats["avg_steps"])
        
        logger.info("=" * 50)
        logger.info("VLM+RLè”åˆè®­ç»ƒæœ€ç»ˆæ‘˜è¦")
        logger.info("=" * 50)
        logger.info(f"æ€»episodeæ•°: {total_episodes}")
        logger.info(f"å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
        logger.info(f"æˆåŠŸç‡: {success_rate:.1f}%")
        logger.info(f"å¹³å‡æ­¥æ•°: {avg_steps:.1f}")
        logger.info("=" * 50)

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("VLM+RLè”åˆè®­ç»ƒç³»ç»Ÿ")
    print("=" * 60)
    
    try:
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = VLMRLTrainer()
        
        # å¼€å§‹è®­ç»ƒ
        trainer.train()
        
        print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
