"""
å¼ºåŒ–å­¦ä¹ æŠ“å–æ™ºèƒ½ä½“
åŸºäºStable-Baselines3çš„PPOç®—æ³•ï¼Œä¸“é—¨ç”¨äºæŠ“å–ä»»åŠ¡
"""

import torch
import torch.nn as nn
import numpy as np
from typing import Dict, Tuple, Optional, Union
import logging
import os

# å°è¯•å¯¼å…¥stable-baselines3
try:
    from stable_baselines3 import PPO
    from stable_baselines3.common.policies import ActorCriticPolicy
    from stable_baselines3.common.vec_env import DummyVecEnv
    from stable_baselines3.common.callbacks import BaseCallback, CheckpointCallback, EvalCallback
    SB3_AVAILABLE = True
    logger = logging.getLogger(__name__)
    logger.info("âœ… Stable-Baselines3 å¯ç”¨")
except ImportError:
    SB3_AVAILABLE = False
    logger = logging.getLogger(__name__)
    logger.warning("âš ï¸  Stable-Baselines3 ä¸å¯ç”¨")

from config import NetworkConfig, TrainingConfig
from environment import PandaGraspingEnv
from training_monitor import TrainingMonitor
# å½’ä¸€åŒ–åŠŸèƒ½ç”±Stable-Baselines3å†…ç½®æä¾›

class GraspingCallback(BaseCallback):
    """æŠ“å–ä»»åŠ¡ä¸“ç”¨å›è°ƒå‡½æ•°"""
    
    def __init__(self, verbose: int = 0, success_threshold: float = 0.8, patience: int = 30):
        super().__init__(verbose)
        self.success_count = 0
        self.total_episodes = 0
        self.episode_rewards = []
        self.episode_lengths = []
        
        # æ—©åœå‚æ•°
        self.success_threshold = success_threshold
        self.patience = patience
        self.recent_successes = []  # è®°å½•æœ€è¿‘çš„æˆåŠŸæƒ…å†µ
        self.should_stop = False
        
    def _on_step(self) -> bool:
        """æ¯æ­¥è°ƒç”¨"""
        # å¦‚æœåº”è¯¥åœæ­¢ï¼Œè¿”å›False
        if self.should_stop:
            return False
            
        # è·å–ç¯å¢ƒä¿¡æ¯
        if hasattr(self.training_env, 'envs'):
            env = self.training_env.envs[0]
        else:
            env = self.training_env
            
        # æ£€æŸ¥æ˜¯å¦å®Œæˆepisode
        if hasattr(env, 'task_state') and env.task_state['grasp_success']:
            self.success_count += 1
            self.total_episodes += 1
            
            # è®°å½•episodeä¿¡æ¯
            if hasattr(env, 'task_state'):
                self.episode_rewards.append(env.task_state.get('episode_reward', 0))
                self.episode_lengths.append(env.task_state.get('episode_steps', 0))
        
        return True
    
    def _on_rollout_end(self) -> None:
        """æ¯ä¸ªrolloutç»“æŸæ—¶è°ƒç”¨"""
        if self.total_episodes > 0:
            success_rate = self.success_count / self.total_episodes
            avg_reward = np.mean(self.episode_rewards) if self.episode_rewards else 0
            avg_length = np.mean(self.episode_lengths) if self.episode_lengths else 0
            
            logger.info(f"Rolloutç»“æŸ - æˆåŠŸç‡: {success_rate:.3f}, å¹³å‡å¥–åŠ±: {avg_reward:.3f}, å¹³å‡æ­¥æ•°: {avg_length:.1f}")
            
            # æ£€æŸ¥æ—©åœæ¡ä»¶
            self._check_early_stopping(success_rate)
            
            # é‡ç½®è®¡æ•°å™¨
            self.success_count = 0
            self.total_episodes = 0
            self.episode_rewards = []
            self.episode_lengths = []
    
    def _check_early_stopping(self, success_rate: float):
        """æ£€æŸ¥æ—©åœæ¡ä»¶"""
        # è®°å½•æœ€è¿‘çš„æˆåŠŸç‡
        self.recent_successes.append(success_rate)
        
        # åªä¿ç•™æœ€è¿‘çš„patienceä¸ªè®°å½•
        if len(self.recent_successes) > self.patience:
            self.recent_successes.pop(0)
        
        # å¦‚æœè®°å½•è¶³å¤Ÿå¤šï¼Œæ£€æŸ¥æ˜¯å¦æ»¡è¶³æ—©åœæ¡ä»¶
        if len(self.recent_successes) >= self.patience:
            # æ£€æŸ¥æœ€è¿‘patienceä¸ªepisodesæ˜¯å¦éƒ½è¾¾åˆ°é˜ˆå€¼
            recent_high_success = [s >= self.success_threshold for s in self.recent_successes[-self.patience:]]
            if all(recent_high_success):
                avg_success_rate = np.mean(self.recent_successes[-self.patience:])
                logger.info(f"ğŸ¯ æ—©åœæ¡ä»¶æ»¡è¶³ï¼")
                logger.info(f"   æœ€è¿‘{self.patience}ä¸ªepisodeséƒ½è¾¾åˆ°æˆåŠŸç‡é˜ˆå€¼")
                logger.info(f"   å¹³å‡æˆåŠŸç‡: {avg_success_rate:.3f}")
                logger.info(f"   æˆåŠŸç‡é˜ˆå€¼: {self.success_threshold}")
                logger.info(f"   è®­ç»ƒå°†æå‰ç»“æŸ")
                self.should_stop = True

class GraspingAgent:
    """
    æŠ“å–ä»»åŠ¡æ™ºèƒ½ä½“
    ä½¿ç”¨PPOç®—æ³•ï¼Œä¸“é—¨ä¼˜åŒ–ç”¨äºæŠ“å–ä»»åŠ¡
    """
    
    def __init__(self, 
                 network_config: NetworkConfig,
                 training_config: TrainingConfig,
                 model_path: str = None):
        """
        åˆå§‹åŒ–æ™ºèƒ½ä½“
        
        Args:
            network_config: ç½‘ç»œé…ç½®
            training_config: è®­ç»ƒé…ç½®
            model_path: é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        """
        self.network_config = network_config
        self.training_config = training_config
        self.model_path = model_path
        
        self.agent = None
        self.env = None
        
        logger.info("æŠ“å–æ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ")
    
    def set_environment(self, env):
        """
        è®¾ç½®è®­ç»ƒç¯å¢ƒ
        
        Args:
            env: Gymnasiumç¯å¢ƒ
        """
        # è®¾ç½®ç¯å¢ƒ
        self.env = env
        
        # åˆ›å»ºè®­ç»ƒç›‘æ§å™¨
        logs_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "logs")
        self.training_monitor = TrainingMonitor(
            log_dir=logs_dir, 
            save_plots=True,
            plot_save_freq=2000  # æ¯500ä¸ªepisodesä¿å­˜ä¸€æ¬¡å›¾è¡¨ï¼ˆé™ä½é¢‘ç‡ï¼‰
        )
        
        # å°†ç›‘æ§å™¨ä¼ é€’ç»™ç¯å¢ƒ
        if hasattr(self.env, 'training_monitor'):
            self.env.training_monitor = self.training_monitor
        
        # åˆ›å»ºå½’ä¸€åŒ–å‘é‡åŒ–ç¯å¢ƒ
        from vec_normalize_wrapper import create_normalized_env
        
        vec_env = create_normalized_env(
            env,
            norm_obs=self.training_config.normalize_observations,
            norm_reward=self.training_config.normalize_rewards,
            clip_obs=self.training_config.norm_obs_clip,
            clip_reward=self.training_config.norm_reward_clip
        )
        
        # ä¼˜åŠ¿å‡½æ•°å½’ä¸€åŒ–ç”±Stable-Baselines3è‡ªåŠ¨å¤„ç†
        self.advantage_normalizer = None
        
        if not SB3_AVAILABLE:
            raise ImportError("Stable-Baselines3ä¸å¯ç”¨ï¼Œæ— æ³•åˆ›å»ºæ™ºèƒ½ä½“")
        
        # è·å–logsç›®å½•è·¯å¾„
        logs_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "logs")
        os.makedirs(logs_dir, exist_ok=True)
        
        # åˆ›å»ºPPOæ™ºèƒ½ä½“
        self.agent = PPO(
            "MlpPolicy",
            vec_env,
            verbose=1,
            learning_rate=self.training_config.learning_rate,
            n_steps=self.training_config.n_steps,
            batch_size=self.training_config.batch_size,
            n_epochs=self.training_config.n_epochs,
            gamma=self.training_config.gamma,
            gae_lambda=self.training_config.gae_lambda,
            clip_range=self.training_config.clip_range,
            ent_coef=self.training_config.ent_coef,
            vf_coef=self.training_config.vf_coef,
            max_grad_norm=self.training_config.max_grad_norm,
            tensorboard_log=logs_dir,  # è®¾ç½®tensorboardæ—¥å¿—è·¯å¾„
            device="cpu",  # å¼ºåˆ¶ä½¿ç”¨CPUï¼Œé¿å…GPUè­¦å‘Š
            policy_kwargs={
                "net_arch": {
                    "pi": self.network_config.policy_hidden_sizes,
                    "vf": self.network_config.value_hidden_sizes
                }
            }
        )
        
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        if self.model_path and os.path.exists(self.model_path):
            self.agent = PPO.load(self.model_path, env=vec_env)
            logger.info(f"åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {self.model_path}")
        
        logger.info("æ™ºèƒ½ä½“ç¯å¢ƒè®¾ç½®å®Œæˆ")
    
    def train(self, total_timesteps: int = None, save_path: str = None):
        """
        è®­ç»ƒæ™ºèƒ½ä½“
        
        Args:
            total_timesteps: æ€»è®­ç»ƒæ­¥æ•°
            save_path: æ¨¡å‹ä¿å­˜è·¯å¾„
        """
        if self.agent is None:
            raise ValueError("æ™ºèƒ½ä½“æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨set_environment")
        
        if total_timesteps is None:
            total_timesteps = self.training_config.total_timesteps
        
        # åˆ›å»ºå›è°ƒå‡½æ•°
        callbacks = []
        
        # æŠ“å–ä»»åŠ¡å›è°ƒï¼ˆåŒ…å«æ—©åœé€»è¾‘ï¼‰
        grasping_callback = GraspingCallback(
            success_threshold=self.training_config.success_threshold,
            patience=self.training_config.patience
        )
        callbacks.append(grasping_callback)
        
        # æ£€æŸ¥ç‚¹å›è°ƒ
        if save_path:
            checkpoint_callback = CheckpointCallback(
                save_freq=self.training_config.save_freq,
                save_path=os.path.dirname(save_path),
                name_prefix=os.path.basename(save_path).replace('.zip', '')
            )
            callbacks.append(checkpoint_callback)
        
        # å¼€å§‹è®­ç»ƒ
        logger.info(f"å¼€å§‹è®­ç»ƒï¼Œæ€»æ­¥æ•°: {total_timesteps}")
        
        # è®¾ç½®episodeç¼–å·è·Ÿè¸ª
        episode_counter = 0
        
        def episode_callback(locals, globals):
            nonlocal episode_counter
            episode_counter += 1
            if hasattr(self.env, 'training_monitor') and self.env.training_monitor:
                self.env.task_state['episode_number'] = episode_counter
        
        # è®­ç»ƒ
        self.agent.learn(
            total_timesteps=total_timesteps,
            callback=callbacks,
            progress_bar=True
        )
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        if save_path:
            self.agent.save(save_path)
            logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
    
    def predict(self, observation: np.ndarray, deterministic: bool = True) -> Tuple[np.ndarray, Optional[np.ndarray]]:
        """
        é¢„æµ‹åŠ¨ä½œ
        
        Args:
            observation: è§‚å¯Ÿ
            deterministic: æ˜¯å¦ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥
            
        Returns:
            Tuple[np.ndarray, Optional[np.ndarray]]: åŠ¨ä½œå’ŒçŠ¶æ€
        """
        if self.agent is None:
            raise ValueError("æ™ºèƒ½ä½“æœªåˆå§‹åŒ–")
        
        return self.agent.predict(observation, deterministic=deterministic)
    
    def evaluate(self, env, n_eval_episodes: int = 10) -> Dict[str, float]:
        """
        è¯„ä¼°æ™ºèƒ½ä½“æ€§èƒ½
        
        Args:
            env: è¯„ä¼°ç¯å¢ƒ
            n_eval_episodes: è¯„ä¼°episodeæ•°é‡
            
        Returns:
            Dict[str, float]: è¯„ä¼°ç»“æœ
        """
        if self.agent is None:
            raise ValueError("æ™ºèƒ½ä½“æœªåˆå§‹åŒ–")
        
        success_count = 0
        total_rewards = []
        episode_lengths = []
        
        for episode in range(n_eval_episodes):
            obs, _ = env.reset()
            episode_reward = 0
            episode_length = 0
            
            while True:
                action, _ = self.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = env.step(action)
                
                episode_reward += reward
                episode_length += 1
                
                if terminated or truncated:
                    break
            
            # æ£€æŸ¥æ˜¯å¦æˆåŠŸ
            if info.get('grasp_success', False):
                success_count += 1
            
            total_rewards.append(episode_reward)
            episode_lengths.append(episode_length)
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        success_rate = success_count / n_eval_episodes
        avg_reward = np.mean(total_rewards)
        avg_length = np.mean(episode_lengths)
        
        results = {
            'success_rate': success_rate,
            'avg_reward': avg_reward,
            'avg_episode_length': avg_length,
            'n_episodes': n_eval_episodes
        }
        
        logger.info(f"è¯„ä¼°ç»“æœ - æˆåŠŸç‡: {success_rate:.3f}, å¹³å‡å¥–åŠ±: {avg_reward:.3f}, å¹³å‡æ­¥æ•°: {avg_length:.1f}")
        
        return results
    
    def save(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        if self.agent is None:
            raise ValueError("æ™ºèƒ½ä½“æœªåˆå§‹åŒ–")
        
        self.agent.save(path)
        logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {path}")
    
    def load(self, path: str):
        """åŠ è½½æ¨¡å‹"""
        if not os.path.exists(path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {path}")
        
        if self.env is None:
            raise ValueError("è¯·å…ˆè®¾ç½®ç¯å¢ƒ")
        
        # åˆ›å»ºå‘é‡åŒ–ç¯å¢ƒ
        vec_env = DummyVecEnv([lambda: self.env])
        
        # åŠ è½½æ¨¡å‹
        self.agent = PPO.load(path, env=vec_env)
        logger.info(f"æ¨¡å‹å·²ä» {path} åŠ è½½")
    
    def get_policy(self):
        """è·å–ç­–ç•¥ç½‘ç»œ"""
        if self.agent is None:
            raise ValueError("æ™ºèƒ½ä½“æœªåˆå§‹åŒ–")
        
        return self.agent.policy
    
    def get_value_function(self):
        """è·å–ä»·å€¼å‡½æ•°"""
        if self.agent is None:
            raise ValueError("æ™ºèƒ½ä½“æœªåˆå§‹åŒ–")
        
        return self.agent.policy.value_net
